{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Introduction to LangChain\n",
    "\n",
    "## Setup\n",
    "#### Follow [README](https://github.com/tirtho/open-ai/blob/main/README.md) and perform setup before running the notebooks\n",
    "\n",
    "Reference : \n",
    "- [Azure Open AI](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview)\n",
    "- [LangChain home page](https://python.langchain.com/docs/get_started/introduction.html)\n",
    "\n",
    "#### Load the API key and relevant Python libaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1685732595135
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Got OPENAI API Key from environment variable\n",
      "Connecting to Open AI returned status as True\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import sys\n",
    "\n",
    "from azure_openai_setup import get_openai_client, get_config_from_os_env, get_chat_completion\n",
    "\n",
    "THE_MODEL = 'gpt-4o'\n",
    "endpoint, key, version = get_config_from_os_env()\n",
    "#print(f\"{endpoint}, {key}, {version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Simple Chatbot example\n",
    "The below example makes direct LangChain library calls so it is better \\\n",
    "for us to understand this LangChain framework.\\\n",
    "In subsequent examples afterwards we will create and use a helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "# The openai.<variables> are already filled up by the above \n",
    "# set_openai_config() helper function called from above cell.\n",
    "# Check that function for more details\n",
    "\n",
    "azureChatClient = AzureChatOpenAI(\n",
    "            openai_api_key = key,\n",
    "            openai_api_base = endpoint,\n",
    "            openai_api_version = version,\n",
    "            deployment_name = THE_MODEL,\n",
    "            temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asking bot to perform as task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'error'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     ChatPromptTemplate,\n\u001b[0;32m      4\u001b[0m     SystemMessagePromptTemplate,\n\u001b[0;32m      5\u001b[0m     AIMessagePromptTemplate,\n\u001b[0;32m      6\u001b[0m     HumanMessagePromptTemplate,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     AIMessage,\n\u001b[0;32m     10\u001b[0m     HumanMessage,\n\u001b[0;32m     11\u001b[0m     SystemMessage\n\u001b[0;32m     12\u001b[0m )\n\u001b[1;32m---> 14\u001b[0m \u001b[43mazureChatClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTranslate this sentence from English to French. I love programming.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chat_models\\base.py:195\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[1;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    190\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    194\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m--> 195\u001b[0m     generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chat_models\\base.py:95\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     94\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[1;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     96\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n\u001b[0;32m     97\u001b[0m generations \u001b[38;5;241m=\u001b[39m [res\u001b[38;5;241m.\u001b[39mgenerations \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chat_models\\base.py:87\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     85\u001b[0m )\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 87\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnew_arg_supported\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     94\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chat_models\\base.py:88\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     83\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     85\u001b[0m )\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 88\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(m, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[0;32m     92\u001b[0m     ]\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     94\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chat_models\\openai.py:354\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    346\u001b[0m     message \u001b[38;5;241m=\u001b[39m _convert_dict_to_message(\n\u001b[0;32m    347\u001b[0m         {\n\u001b[0;32m    348\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: inner_completion,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    351\u001b[0m         }\n\u001b[0;32m    352\u001b[0m     )\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations\u001b[38;5;241m=\u001b[39m[ChatGeneration(message\u001b[38;5;241m=\u001b[39mmessage)])\n\u001b[1;32m--> 354\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chat_models\\openai.py:296\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompletion_with_retry\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    295\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m     retry_decorator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_retry_decorator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chat_models\\openai.py:285\u001b[0m, in \u001b[0;36mChatOpenAI._create_retry_decorator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    277\u001b[0m max_seconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m60\u001b[39m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# Wait 2^x * 1 second between each retry starting with\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m# 4 seconds, then up to 10 seconds, then 10 seconds afterwards\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retry(\n\u001b[0;32m    281\u001b[0m     reraise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    282\u001b[0m     stop\u001b[38;5;241m=\u001b[39mstop_after_attempt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries),\n\u001b[0;32m    283\u001b[0m     wait\u001b[38;5;241m=\u001b[39mwait_exponential(multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39mmin_seconds, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mmax_seconds),\n\u001b[0;32m    284\u001b[0m     retry\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m--> 285\u001b[0m         retry_if_exception_type(\u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[38;5;241m.\u001b[39mTimeout)\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;241m|\u001b[39m retry_if_exception_type(openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mAPIError)\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;241m|\u001b[39m retry_if_exception_type(openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mAPIConnectionError)\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;241m|\u001b[39m retry_if_exception_type(openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mRateLimitError)\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;241m|\u001b[39m retry_if_exception_type(openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mServiceUnavailableError)\n\u001b[0;32m    290\u001b[0m     ),\n\u001b[0;32m    291\u001b[0m     before_sleep\u001b[38;5;241m=\u001b[39mbefore_sleep_log(logger, logging\u001b[38;5;241m.\u001b[39mWARNING),\n\u001b[0;32m    292\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'openai' has no attribute 'error'"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "azureChatClient([HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate completions for multiple sets of messaages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'adore la programmation.\n",
      "J'adore l'intelligence artificielle.\n"
     ]
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "        HumanMessage(content=\"I love programming.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "        HumanMessage(content=\"I love artificial intelligence.\")\n",
    "    ],\n",
    "]\n",
    "result = azureChatClient.generate(batch_messages)\n",
    "for generation in result.generations:\n",
    "    print(generation[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "The concept of templates make life so much easier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Prompt Engineer creates the ChatPrompt object\n",
    "\n",
    "and hands over the aChatPrompt to your Application Dev Engineer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"You are a helpful assistance that translates to {target_language}.\"\n",
    "aSystemPrompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "human_template = \"{text}\"\n",
    "aHumanPrompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "aChatPrompt = ChatPromptTemplate.from_messages([aSystemPrompt, aHumanPrompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your App Dev Engineer receives aChatPrompt\n",
    "Populates that prompt with user input and receives completion from the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "theCompletion = azureChatClient(aChatPrompt.format_prompt(target_language=\"English Pirate\", \n",
    "                                text=\"Hello Sir! Would you like to buy a bottle of rum from \\\n",
    "                                the Captain of the Black Pearl?\").to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy matey! Aye, I would be interested in purchasing a bottle of rum from the Captain of the Black Pearl. Pray tell, what be the price for such a fine bottle of grog?\n"
     ]
    }
   ],
   "source": [
    "print(theCompletion.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parser\n",
    "Helps you get the completion parsed in your desired format. Different types of output parsers are\n",
    "\n",
    "- PydanticOutputParser\n",
    "- CommaSeparatedListOutputParser\n",
    "- Datetime\n",
    "- Enum\n",
    "- OutputFixingParser\n",
    "- RetryOutputParser\n",
    "- Structured Output Parser\n",
    "\n",
    "Let's checkout the Structured Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's take this actual quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_quote_struct = {\n",
    "    'auto': True,\n",
    "    'premium': '$200',\n",
    "    'driver': 'TR BARARI',\n",
    "    'vin': 'JH4DA9460LS000685',\n",
    "    'liability':'$100000',\n",
    "    'collision_deduct':'$250',\n",
    "    'comprehensive_deduct':'$100',\n",
    "    'personal_injury':True,\n",
    "    'uninsured':True,\n",
    "    'underinsured':True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is the example quote in an email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_insurance_quote_text = f\"\"\"\n",
    "Dear Mr TR Barari, \\\n",
    "\n",
    "I would like to take this opportunity to congratulate you \\\n",
    "on being approved for your auto insurance from Reliable Insurance Inc. \\\n",
    " \\\n",
    "Here are the details of the quote. \\\n",
    " \\\n",
    "    Monthly premium is $200 \\\n",
    "    Driver Name is TR BARARI \\\n",
    "    Auto VIN Number is JH4DA9460LS000685 \\\n",
    "    Coverage for Liability Insurance is $100000 \\\n",
    "    Collision is covered with a per claim deductible of $250 \\\n",
    "    Comprehensive insurance is included with a per claim deductible of $100 \\\n",
    "    Personal_injury is covered \\\n",
    "    Includes full coverage against uninsured and underinsured motorist \\\n",
    "    Coverage Start Date 06/01/2023 \\\n",
    " \\\n",
    "Please email back with your acceptance of this offer in the next 3 days \\\n",
    "for this to be effective. \\\n",
    " \\\n",
    "Regards \\\n",
    " \\\n",
    "John Doe \\\n",
    "Vice President \\\n",
    "Reliable Insurance Inc.\\\n",
    "1 Main Street, NY 12345 \\\n",
    "Ph: +1 (234) 567-9876\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The template from the Prompt Engineer with instructions to process quote emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_insurance_quote_template = \"\"\"\n",
    "Extract from {text} based on the instructions in {format_instructions}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template=auto_insurance_quote_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's use the LangChain output parser to parse insurance quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "auto_schema = ResponseSchema(name=\"auto\",\n",
    "                             description=\"Was this an Auto Insurance Quote? Answer as True or False\")\n",
    "premium_schema = ResponseSchema(name=\"premium\",\n",
    "                             description=\"How much is the monthly premium?\")\n",
    "driver_schema = ResponseSchema(name=\"driver\",\n",
    "                             description=\"What is the name of the driver?\")\n",
    "vin_schema = ResponseSchema(name=\"vin\",\n",
    "                             description=\"What is the VIN Number of the auto?\")\n",
    "liability_schema = ResponseSchema(name=\"liability\",\n",
    "                             description=\"How much is the liability coverage?\")\n",
    "collision_deduct_schema = ResponseSchema(name=\"collision_deduct\",\n",
    "                             description=\"How much is the collision deductible per claim?\")\n",
    "comprehensive_deduct_schema = ResponseSchema(name=\"comprehensive_deduct\",\n",
    "                             description=\"How much is the comprehensive deductible per claim?\")\n",
    "uninsured_schema = ResponseSchema(name=\"uninsured\",\n",
    "                             description=\"Is uninsured motorist coverage included? Answer as True or False\")\n",
    "underinsured_schema = ResponseSchema(name=\"underinsured\",\n",
    "                             description=\"Is underinsured motorist coverage included? Answer as True or False\")\n",
    "\n",
    "insurance_quote_schemas = [auto_schema, premium_schema, driver_schema, \n",
    "                    vin_schema, liability_schema, collision_deduct_schema, \n",
    "                    comprehensive_deduct_schema, uninsured_schema, underinsured_schema]\n",
    "\n",
    "insurance_quote_parser = StructuredOutputParser.from_response_schemas(insurance_quote_schemas)\n",
    "\n",
    "auto_insurance_quote_format_instructions = insurance_quote_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Developer creates the message, sends it to the model and gets the response back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = prompt.format_messages(text=auto_insurance_quote_text,\n",
    "                                 format_instructions=auto_insurance_quote_format_instructions)\n",
    "response_from_model = azureChatClient(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the LangChain parser to parse the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_quote_struct = insurance_quote_parser.parse(response_from_model.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auto': True,\n",
       " 'premium': '$200',\n",
       " 'driver': 'TR BARARI',\n",
       " 'vin': 'JH4DA9460LS000685',\n",
       " 'liability': '$100000',\n",
       " 'collision_deduct': '$250',\n",
       " 'comprehensive_deduct': '$100',\n",
       " 'uninsured': True,\n",
       " 'underinsured': True}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_quote_struct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test if the expected quote is same as the quote extracted by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: The expected quote is same as the output quote extracted by the Azure OpenAI model\n"
     ]
    }
   ],
   "source": [
    "# Check if the quoted $ is same in both\n",
    "\n",
    "if output_quote_struct.get('premium') == actual_quote_struct.get('premium'):\n",
    "    print(\"SUCCESS: The expected quote is same as the output quote extracted by the Azure OpenAI model\")\n",
    "else:\n",
    "    print(\"FAILED: The expected and output quote are not same\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
