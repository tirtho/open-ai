{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Memory\n",
    "\n",
    "## Setup\n",
    "#### Follow [README](https://github.com/tirtho/open-ai/blob/main/README.md) and perform setup before running the notebooks\n",
    "\n",
    "Reference : \n",
    "- [Azure Open AI](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview)\n",
    "- [LangChain home page](https://python.langchain.com/docs/get_started/introduction.html)\n",
    "\n",
    "#### Load the API key and relevant Python libaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1685732595135
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got Azure OpenAI Credentials from Azure Key Vault with Azure CLI Auth\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import sys\n",
    "\n",
    "from azure_openai_setup import set_openai_config, get_openai_global_config_parameters \n",
    "\n",
    "set_openai_config()\n",
    "\n",
    "theOpenAIParams, modelName, modelDeploymentName = get_openai_global_config_parameters()\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "# The openai.<variables> are already filled up by the above \n",
    "# set_openai_config() helper function called from above cell.\n",
    "# Check that function for more details\n",
    "\n",
    "azureChatClient = AzureChatOpenAI(\n",
    "            openai_api_key = theOpenAIParams.api_key,\n",
    "            openai_api_base = theOpenAIParams.api_base,\n",
    "            openai_api_version = theOpenAIParams.api_version,\n",
    "            deployment_name = modelDeploymentName,\n",
    "            temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add memory to an LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a chatbot having a conversation with a human.\n",
      "\n",
      "\n",
      "\n",
      "Human: Hi there! My name is Joe Smith.\n",
      "\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Joe Smith! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import OpenAI, LLMChain, PromptTemplate\n",
    "\n",
    "template = \"\"\"You are a chatbot having a conversation with a human.\n",
    "\n",
    "{chat_history_from_memory}\n",
    "\n",
    "Human: {human_input}\n",
    "\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "                input_variables=[\"chat_history_from_memory\", \"human_input\"], \n",
    "                template=template\n",
    "            )\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history_from_memory\")\n",
    "\n",
    "langChainConversationWithMemory = LLMChain(\n",
    "                                    llm=azureChatClient,\n",
    "                                    prompt=prompt,\n",
    "                                    verbose=True,\n",
    "                                    memory=memory\n",
    "                                  )\n",
    "langChainConversationWithMemory.predict(human_input=\"Hi there! My name is Joe Smith.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View what is in the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi there! My name is Joe Smith.\n",
      "AI: Hello Joe Smith! It's nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View what is in the memory (in json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history_from_memory': \"Human: Hi there! My name is Joe Smith.\\nAI: Hello Joe Smith! It's nice to meet you. How can I assist you today?\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add more data to the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi there! My name is Joe Smith.\n",
      "AI: Hello Joe Smith! It's nice to meet you. How can I assist you today?\n",
      "Human: How are you doing my friend?\n",
      "AI: I am well. Thank you for asking.\n"
     ]
    }
   ],
   "source": [
    "memory.save_context({\"Human\": \"How are you doing my friend?\"}, {\"AI\": \"I am well. Thank you for asking.\"})\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a chatbot having a conversation with a human.\n",
      "\n",
      "Human: Hi there! My name is Joe Smith.\n",
      "AI: Hello Joe Smith! It's nice to meet you. How can I assist you today?\n",
      "Human: How are you doing my friend?\n",
      "AI: I am well. Thank you for asking.\n",
      "\n",
      "Human: Can you tell me my last name?\n",
      "\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, your last name is Smith.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langChainConversationWithMemory.predict(human_input=\"Can you tell me my last name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep Last k interactions in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3584280531.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[17], line 8\u001b[1;36m\u001b[0m\n\u001b[1;33m    (\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Using the same prompt template from above\n",
    "# just with a different memory buffer\n",
    "\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "memory = ConversationBufferWindowMemory(memory_key=\"chat_history_from_memory\", k=2)\n",
    "\n",
    "langChainConversationWithMemory = LLMChain(\n",
    "                                    llm=azureChatClient,\n",
    "                                    prompt=prompt,\n",
    "                                    verbose=True,\n",
    "                                    memory=memory\n",
    "                                  )\n",
    "\n",
    "langChainConversationWithMemory.predict(human_input=\"Hi there! My name is Joan Johnson.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a chatbot having a conversation with a human.\n",
      "\n",
      "Human: Hi there! My name is Joan Johnson.\n",
      "AI: Hello Joan Johnson! It's nice to meet you. How can I assist you today?\n",
      "\n",
      "Human: Can you tell me my name?\n",
      "\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, your name is Joan Johnson.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your name is still in memory (as last 2 interactions are stored)\n",
    "# So, the model should be able to repond back with your name correctly.\n",
    "langChainConversationWithMemory.predict(human_input=\"Can you tell me my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a chatbot having a conversation with a human.\n",
      "\n",
      "Human: Hi there! My name is Joan Johnson.\n",
      "AI: Hello Joan Johnson! It's nice to meet you. How can I assist you today?\n",
      "Human: Can you tell me my name?\n",
      "AI: Yes, your name is Joan Johnson.\n",
      "\n",
      "Human: How are you doing?\n",
      "\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have emotions like humans do, but I'm functioning properly and ready to assist you with any questions or tasks you may have. How can I help you today?\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets add some more interactions\n",
    "langChainConversationWithMemory.predict(human_input=\"How are you doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a chatbot having a conversation with a human.\n",
      "\n",
      "Human: Can you tell me my name?\n",
      "AI: Yes, your name is Joan Johnson.\n",
      "Human: How are you doing?\n",
      "AI: As an AI, I don't have emotions like humans do, but I'm functioning properly and ready to assist you with any questions or tasks you may have. How can I help you today?\n",
      "\n",
      "Human: What is the capital of USA?\n",
      "\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of the USA is Washington D.C.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding one more interaction\n",
    "langChainConversationWithMemory.predict(human_input=\"What is the capital of USA?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a chatbot having a conversation with a human.\n",
      "\n",
      "Human: How are you doing?\n",
      "AI: As an AI, I don't have emotions like humans do, but I'm functioning properly and ready to assist you with any questions or tasks you may have. How can I help you today?\n",
      "Human: What is the capital of USA?\n",
      "AI: The capital of the USA is Washington D.C.\n",
      "\n",
      "Human: Can you tell me my name?\n",
      "\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have access to that information. Could you please tell me your name?\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the first interaction is out of memory buffer\n",
    "# So, the model can't tell your name anymore\n",
    "langChainConversationWithMemory.predict(human_input=\"Can you tell me my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Memory types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Keep max token limit in memory\n",
    "Use ConversationTokenBufferMemory(llm, max_token_limit)\n",
    "#### Create custom memory class\n",
    "[Custom Memory Class Example](https://python.langchain.com/docs/modules/memory/how_to/custom_memory)\n",
    "#### 2. Keep memory of given facts about specific entities in a conversation with Entity Memory\n",
    "[Entity memory example](https://python.langchain.com/docs/modules/memory/how_to/entity_summary_memory)\n",
    "#### 3. Conversation Summary Memory\n",
    "\n",
    "Conversation summary memory summarizes the conversation as it happens and stores the current summary in memory. This memory can then be used to inject the summary of the conversation so far into a prompt/chain. This memory is most useful for longer conversations, where keeping the past message history in the prompt verbatim would take up too many tokens.\n",
    "\n",
    "[Detailed examples](https://python.langchain.com/docs/modules/memory/how_to/summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation SummaryBufferMemory\n",
    "\n",
    "It keeps a buffer of recent interactions in memory, but rather than just completely flushing old interactions it compiles them into a summary and uses both. Unlike the previous implementation though, it uses token length rather than number of interactions to determine when to flush interactions. [Details](https://python.langchain.com/docs/modules/memory/how_to/summary_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, this is Joe from Massachusetts.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Joe from Massachusetts! It's great to chat with you. How can I assist you today?\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "                llm=azureChatClient,\n",
    "                max_token_limit=50\n",
    "            )\n",
    "\n",
    "conversationWithSummaryChain = ConversationChain(\n",
    "                                    llm = azureChatClient,\n",
    "                                    memory = memory,\n",
    "                                    verbose = True\n",
    "                                )\n",
    "\n",
    "conversationWithSummaryChain.predict(input=\"Hi, this is Joe from Massachusetts.\")\n",
    "# The message so far is within the token limit, so no summarization yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, this is Joe from Massachusetts.\n",
      "AI: Hello Joe from Massachusetts! It's great to chat with you. How can I assist you today?\n",
      "Human: Will you help me get a summary on a topic?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Of course, I'd be happy to help! What topic are you interested in?\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversationWithSummaryChain.predict(input=\"Will you help me get a summary on a topic?\")\n",
    "# The message so far is within the token limit, so no summarization yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now it should summarize the previous interactions and continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human introduces themselves as Joe from Massachusetts and the AI greets them warmly, asking how they can assist.\n",
      "Human: Will you help me get a summary on a topic?\n",
      "AI: Of course, I'd be happy to help! What topic are you interested in?\n",
      "Human: Can you write a summary of Large Language Models?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Absolutely! Large Language Models are a type of artificial intelligence that use deep learning algorithms to analyze and understand human language. These models are trained on massive amounts of text data, allowing them to generate human-like responses to questions and even create original content. Some of the most well-known examples of Large Language Models include GPT-3 and BERT. These models have a wide range of applications, from chatbots and virtual assistants to language translation and content creation. However, there are also concerns about the potential misuse of these models, such as the spread of misinformation or the creation of deepfakes. Overall, Large Language Models are a powerful tool in the field of AI, but their development and use must be carefully monitored and regulated.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversationWithSummaryChain.predict(input=\"Can you write a summary of Large Language Models?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector store-backed memory\n",
    "\n",
    "This memory type stores memories in a VectorDB and queries the top-K most \"salient\" docs every time it is called.\n",
    "\n",
    "This differs from most of the other Memory classes in that it doesn't explicitly track the order of interactions.\n",
    "\n",
    "[Examples](https://python.langchain.com/docs/modules/memory/how_to/vectorstore_retriever_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
